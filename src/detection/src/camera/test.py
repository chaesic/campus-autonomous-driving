import cv2
import torch
import numpy as np

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸ”¹ ëª¨ë¸ ê²½ë¡œ
DET_MODEL = "/home/jaemin/yolopose/model/yolo11s.pt"
POSE_MODEL = "/home/jaemin/yolopose/model/yolo11s-pose.pt"
VIDEO_PATH = "/home/jaemin/JAAD/JAAD_clips/video_0006.mp4"

# ğŸ”¹ ì„ê³„ê°’ (ë¯¼ê°ë„ ë†’ì„)
TH_NORMAL = 0.15
TH_DANGER = 0.22

from ultralytics import YOLO

print(f"ğŸ”¹ Using device: {DEVICE}")
print(f"ğŸ”¹ Loading detection model from {DET_MODEL}")
yolo_model = YOLO(DET_MODEL).to(DEVICE)

print(f"ğŸ”¹ Loading pose model from {POSE_MODEL}")
pose_model = YOLO(POSE_MODEL).to(DEVICE)

# ğŸ”¹ ê±°ë¦¬ ì¶”ì • í•¨ìˆ˜ (ìƒëŒ€ì  ìœ„í—˜ë„ ê¸°ë°˜)
def estimate_relative_distance(bbox, frame_height):
    x1, y1, x2, y2 = bbox
    h = y2 - y1
    cy = y2 / frame_height  # í•˜ë‹¨ë¶€ ìœ„ì¹˜ ë¹„ìœ¨ (0~1)
    size_factor = h / frame_height  # bbox ë†’ì´ ë¹„ìœ¨ (0~1)
    # í™”ë©´ í•˜ë‹¨ì— ìˆê³ , bboxê°€ í¬ë©´ ë” ê°€ê¹Œì›€ (ê°’ ì‘ì„ìˆ˜ë¡ ìœ„í—˜)
    distance_score = 1.0 - (0.6 * cy + 0.4 * size_factor)
    return max(0.0, min(1.0, distance_score))  # 0~1 ì‚¬ì´ ì •ê·œí™”

# ğŸ”¹ ìœ„í—˜ë„ íŒì •
def get_pedestrian_state(distance_score):
    if distance_score < TH_NORMAL:
        return "danger"
    elif distance_score < TH_DANGER:
        return "normal"
    else:
        return "safe"

# ğŸ”¹ ìƒ‰ìƒ ì •ì˜
COLORS = {"safe": (0, 255, 0), "normal": (0, 165, 255), "danger": (0, 0, 255)}

# ğŸ”¹ Pose skeleton ì—°ê²° ì •ì˜ (COCO keypoints ê¸°ë°˜ ì˜ˆì‹œ)
POSE_CONNECTIONS = [
    (5, 7), (7, 9),   # ì™¼íŒ”
    (6, 8), (8, 10),  # ì˜¤ë¥¸íŒ”
    (11, 13), (13, 15),  # ì™¼ë‹¤ë¦¬
    (12, 14), (14, 16),  # ì˜¤ë¥¸ë‹¤ë¦¬
    (5, 6), (11, 12),  # ì–´ê¹¨, ì—‰ë©ì´
    (5, 11), (6, 12)   # ëª¸í†µ ì—°ê²°
]

# ğŸ”¹ skeleton ê·¸ë¦¬ê¸°
def draw_skeleton(frame, keypoints, color):
    for (x, y, conf) in keypoints:
        if conf > 0.3:
            cv2.circle(frame, (int(x), int(y)), 3, color, -1)
    for (i, j) in POSE_CONNECTIONS:
        if i < len(keypoints) and j < len(keypoints):
            if keypoints[i][2] > 0.3 and keypoints[j][2] > 0.3:
                pt1 = (int(keypoints[i][0]), int(keypoints[i][1]))
                pt2 = (int(keypoints[j][0]), int(keypoints[j][1]))
                cv2.line(frame, pt1, pt2, color, 2)

cap = cv2.VideoCapture(VIDEO_PATH)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_resized = cv2.resize(frame, (960, 540))
    results = yolo_model(frame_resized)
    detections = results.xyxy[0].cpu().numpy()

    # âœ… ë³´í–‰ìë§Œ í•„í„°ë§ í›„ ìƒìœ„ 5ëª…ë§Œ ìœ ì§€
    pedestrians = [d for d in detections if int(d[5]) == 0][:5]

    for det in pedestrians:
        x1, y1, x2, y2, conf, cls = det
        bbox = [int(x1), int(y1), int(x2), int(y2)]

        distance_score = estimate_relative_distance(bbox, frame_resized.shape[0])
        state = get_pedestrian_state(distance_score)

        color = COLORS[state]
        cv2.rectangle(frame_resized, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
        label = f"{state.upper()} ({distance_score:.2f})"
        cv2.putText(frame_resized, label, (bbox[0], bbox[1] - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        # âœ… normal/danger ë³´í–‰ìëŠ” pose ì¶”ì¶œ ë° skeleton ì‹œê°í™”
        if state in ["normal", "danger"]:
            crop = frame_resized[bbox[1]:bbox[3], bbox[0]:bbox[2]]
            pose_result = pose_model(crop)

            if hasattr(pose_result, 'keypoints'):
                keypoints = pose_result.keypoints[0].cpu().numpy()
                # bbox offset ì ìš©
                keypoints[:, 0] += bbox[0]
                keypoints[:, 1] += bbox[1]
                draw_skeleton(frame_resized, keypoints, color)

    cv2.imshow("Pedestrian Risk Detection", frame_resized)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
